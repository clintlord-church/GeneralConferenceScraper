{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbjYL0qTLjEc8hL2K1ojTC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukejoneslj/GeneralConferenceScraper/blob/main/ConferenceScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You don't need to read all of this if you don't want to. Just \"run all\" if you want\" and wait for it to download."
      ],
      "metadata": {
        "id": "PoZRQbASYwYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Required Libraries\n",
        "\n",
        "This cell installs the necessary Python libraries for web scraping, working with HTML content, and data manipulation.\n",
        "\n",
        "- `requests`: Used to fetch HTML content from web pages.\n",
        "- `beautifulsoup4`: Parses and extracts content from the HTML.\n",
        "- `PyPDF2`: If you need to work with PDF files.\n"
      ],
      "metadata": {
        "id": "225EVrFfYaRd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHwZi0Axx2ZD"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4 PyPDF2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Web Scraping Conference Talks\n",
        "\n",
        "This block contains helper functions for scraping LDS General Conference talks from the Church's website.\n",
        "\n",
        "- `get_soup()`: Sends a request and parses the HTML from a given URL.\n",
        "- `is_decade_page()`: Identifies if the URL is a decade selection page.\n",
        "- `scrape_conference_pages()`: Fetches URLs for each conference (April/October) from the main page.\n",
        "- `scrape_talk_urls()`: Retrieves URLs for individual talks from a specific conference.\n",
        "- `scrape_talk_data()`: Scrapes the detailed data for each talk, such as title, speaker, calling, year, and season.\n"
      ],
      "metadata": {
        "id": "3Z3FcXjAYgBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import time\n",
        "\n",
        "def get_soup(url):\n",
        "    \"\"\"Create a tree structure (BeautifulSoup) out of a GET request's HTML.\"\"\"\n",
        "    try:\n",
        "        r = requests.get(url, allow_redirects=True)\n",
        "        r.raise_for_status()\n",
        "        print(f\"Successfully fetched {r.url}\")\n",
        "        return BeautifulSoup(r.content, \"html5lib\")\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def is_decade_page(url):\n",
        "    \"\"\"Check if a page is a decade selection page.\"\"\"\n",
        "    return bool(re.search(r\"/study/general-conference/\\d{4}\\d{4}\", url))\n",
        "\n",
        "def scrape_conference_pages(main_page_url):\n",
        "    \"\"\"Retrieve a list of URLs for each conference (year/month) from the main page.\"\"\"\n",
        "    soup = get_soup(main_page_url)\n",
        "    if soup is None:\n",
        "        print(f\"Failed to fetch content from {main_page_url}\")\n",
        "        return []\n",
        "\n",
        "    all_conference_links = []\n",
        "\n",
        "    # Find all the links to individual conferences or decades\n",
        "    links = [\n",
        "        \"https://www.churchofjesuschrist.org\" + a[\"href\"]\n",
        "        for a in soup.find_all(\"a\", href=True)\n",
        "        if re.search(r\"/study/general-conference/(\\d{4}/(04|10)|\\d{4}\\d{4})\", a[\"href\"])\n",
        "    ]\n",
        "\n",
        "    for link in links:\n",
        "        if is_decade_page(link):\n",
        "            # Handle decade page\n",
        "            decade_soup = get_soup(link)\n",
        "            if decade_soup:\n",
        "                year_links = [\n",
        "                    \"https://www.churchofjesuschrist.org\" + a[\"href\"]\n",
        "                    for a in decade_soup.find_all(\"a\", href=True)\n",
        "                    if re.search(r\"/study/general-conference/\\d{4}/(04|10)\", a[\"href\"])\n",
        "                ]\n",
        "                all_conference_links.extend(year_links)\n",
        "        else:\n",
        "            all_conference_links.append(link)\n",
        "\n",
        "    print(f\"Total conference links found: {len(all_conference_links)}\")\n",
        "    print(\"Sample conference links:\", all_conference_links[:5])\n",
        "    return all_conference_links\n",
        "\n",
        "def scrape_talk_urls(conference_url):\n",
        "    \"\"\"Retrieve a list of URLs for each talk in a specific conference.\"\"\"\n",
        "    soup = get_soup(conference_url)\n",
        "    if soup is None:\n",
        "        return []\n",
        "\n",
        "    talk_links = [\n",
        "        \"https://www.churchofjesuschrist.org\" + a[\"href\"]\n",
        "        for a in soup.find_all(\"a\", href=True)\n",
        "        if re.search(r\"/study/general-conference/\\d{4}/(04|10)/.*\", a[\"href\"])\n",
        "    ]\n",
        "\n",
        "    # Remove duplicate links and session links\n",
        "    talk_links = list(set(talk_links))\n",
        "    talk_links = [link for link in talk_links if not link.endswith(\"session?lang=eng\")]\n",
        "\n",
        "    print(f\"Found {len(talk_links)} talk links in {conference_url}\")\n",
        "    if talk_links:\n",
        "        print(\"Sample talk links:\", talk_links[:3])\n",
        "    return talk_links\n",
        "\n",
        "def scrape_talk_data(url):\n",
        "    \"\"\"Scrapes a single talk for data such as: title, conference, calling, speaker, content.\"\"\"\n",
        "    try:\n",
        "        soup = get_soup(url)\n",
        "        if soup is None:\n",
        "            return {}\n",
        "\n",
        "        title_tag = soup.find(\"h1\", {\"id\": \"title1\"})\n",
        "        title = title_tag.text.strip() if title_tag else \"No Title Found\"\n",
        "\n",
        "        conference_tag = soup.find(\"p\", {\"class\": \"subtitle\"})\n",
        "        conference = conference_tag.text.strip() if conference_tag else \"No Conference Found\"\n",
        "\n",
        "        author_tag = soup.find(\"p\", {\"class\": \"author-name\"})\n",
        "        speaker = author_tag.text.strip() if author_tag else \"No Speaker Found\"\n",
        "\n",
        "        calling_tag = soup.find(\"p\", {\"class\": \"author-role\"})\n",
        "        calling = calling_tag.text.strip() if calling_tag else \"No Calling Found\"\n",
        "\n",
        "        content_array = soup.find(\"div\", {\"class\": \"body-block\"})\n",
        "        content = \"\\n\\n\".join(paragraph.text.strip() for paragraph in content_array.find_all(\"p\")) if content_array else \"No Content Found\"\n",
        "\n",
        "        footnotes = \"\\n\".join(\n",
        "            f\"{idx}. {note.text.strip()}\" for idx, note in enumerate(soup.find_all(\"li\", {\"class\": \"study-note\"}), start=1)\n",
        "        ) if soup.find_all(\"li\", {\"class\": \"study-note\"}) else \"No Footnotes Found\"\n",
        "\n",
        "        year = re.search(r'/(\\d{4})/', url).group(1)\n",
        "        season = \"April\" if \"/04/\" in url else \"October\"\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"speaker\": speaker,\n",
        "            \"calling\": calling,\n",
        "            \"conference\": conference,\n",
        "            \"year\": year,\n",
        "            \"season\": season,\n",
        "            \"url\": url,\n",
        "            \"talk\": content,\n",
        "            \"footnotes\": footnotes,\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to scrape {url}: {e}\")\n",
        "        return {}\n",
        "\n",
        "def main_scrape_process():\n",
        "    main_url = \"https://www.churchofjesuschrist.org/study/general-conference?lang=eng\"\n",
        "    conference_urls = scrape_conference_pages(main_url)\n",
        "\n",
        "    all_talk_urls = []\n",
        "    for conference_url in conference_urls:\n",
        "        all_talk_urls.extend(scrape_talk_urls(conference_url))\n",
        "\n",
        "    print(f\"Total talks found: {len(all_talk_urls)}\")\n",
        "\n",
        "    conference_talks = []\n",
        "    for i, url in enumerate(all_talk_urls):\n",
        "        print(f\"Scraping talk {i+1}/{len(all_talk_urls)}: {url}\")\n",
        "        talk_data = scrape_talk_data(url)\n",
        "        if talk_data:\n",
        "            conference_talks.append(talk_data)\n",
        "\n",
        "    conference_df = pd.DataFrame(conference_talks)\n",
        "\n",
        "    for col in conference_df.columns:\n",
        "        conference_df[col] = conference_df[col].apply(lambda x: unicodedata.normalize(\"NFD\", x) if isinstance(x, str) else x)\n",
        "        conference_df[col] = conference_df[col].apply(lambda x: x.replace(\"\\t\", \"\") if isinstance(x, str) else x)\n",
        "\n",
        "    conference_df.to_csv(\"conference_talks.csv\", index=False)\n",
        "    print(\"Scraping complete. Data saved to 'conference_talks.csv'.\")\n",
        "\n",
        "    conference_df.to_json(\"conference_talks.json\", orient=\"records\", indent=4)\n",
        "    print(\"Data also saved to 'conference_talks.json'.\")\n",
        "\n",
        "start = time.time()\n",
        "main_scrape_process()\n",
        "end = time.time()\n",
        "print(f\"Total time taken: {end - start} seconds\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-0Eh87T88k-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('conference_talks.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "xUHjf5EE9Oz-",
        "outputId": "6eaaffdd-1fdb-4f79-97a1-a32082bff4ea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2bf64bb5-a6ca-49cf-b426-dcd7f7c95318\", \"conference_talks.csv\", 42773420)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean Conference Talks Data\n",
        "\n",
        "This code block performs several cleaning operations on the scraped data:\n",
        "- Removes rows with \"Church Auditing Department\" in the calling.\n",
        "- Removes unnecessary columns, like \"conference\" and \"footnotes.\"\n",
        "- Standardizes titles for speakers and callings (e.g., \"Quorum of the 12\" and \"Seventy\").\n",
        "- Ensures uniformity across callings and speaker titles.\n",
        "- Saves the cleaned data to a CSV file for easier analysis and sharing.\n",
        "\n",
        "You can run this after scraping the data to ensure consistency and uniformity across the dataset.\n"
      ],
      "metadata": {
        "id": "KwxzUhLCYp3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def clean_conference_data(file_path):\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 1: Remove rows with \"Church Auditing Department\" in the \"calling\" column\n",
        "    df = df[~df['calling'].str.contains(\"Church Auditing Department\", na=False)]\n",
        "\n",
        "    # Step 2: Remove the \"conference\" column\n",
        "    df = df.drop(columns=['conference'])\n",
        "\n",
        "    # Step 3: Modify callings to standardize common names\n",
        "    # Replace \"Quorum of the Twelve Apostles\", \"twelve\", or \"12\" with \"Quorum of the 12\"\n",
        "    df['calling'] = df['calling'].str.replace(r'Q_of_12|twelve|12|Council of the 12', 'Quorum of the 12', regex=True)\n",
        "\n",
        "    # Replace all variations of \"Seventy\" (including Quorum, Assistant, Former, Released) with \"Seventy\"\n",
        "    df['calling'] = df['calling'].str.replace(r'Q_of_70|70|Assistant to the Q_of_12|First Council of the Seventy|Presidency of the First Q_of_70|Emeritus member of the Seventy|Released Member of the Seventy', 'Seventy', regex=True)\n",
        "\n",
        "    # Standardize \"President of the Church\" across all variations\n",
        "    df['calling'] = df['calling'].str.replace(r'President of The Church of Jesus Christ of Latter-day Saints|President of the Church', 'President of the Church', regex=True)\n",
        "\n",
        "    # Step 4: Remove rows with \"Presented by\" in the speaker column\n",
        "    df = df[~df['speaker'].str.contains(\"Presented by\", na=False)]\n",
        "\n",
        "    # Step 5: Remove \"morning\", \"afternoon\", \"evening\" session titles (non-talk rows)\n",
        "    df = df[~df['title'].str.contains(r'morning|afternoon|evening', case=False, na=False)]\n",
        "\n",
        "    # Step 6: Standardize \"Former\" or variations in calling titles\n",
        "    df['calling'] = df['calling'].str.replace(r'Former member of the Seventy', 'Seventy', regex=True)\n",
        "\n",
        "    # Step 7: Remove \"Elder\", \"President\", \"Sister\", \"Brother\" from speaker names\n",
        "    df['speaker'] = df['speaker'].str.replace(r'Elder|President|Sister|Brother', '', regex=True).str.strip()\n",
        "\n",
        "    # Step 8: Remove the \"footnotes\" column as it was not needed\n",
        "    if 'footnotes' in df.columns:\n",
        "        df = df.drop(columns=['footnotes'])\n",
        "\n",
        "    # Save the cleaned data to CSV\n",
        "    df.to_csv(\"cleaned_conference_talks.csv\", index=False)\n",
        "    print(\"Data cleaned and saved to 'cleaned_conference_talks.csv'.\")\n",
        "\n",
        "# Example usage\n",
        "clean_conference_data('conference_talks.csv')\n"
      ],
      "metadata": {
        "id": "Lo1nq2mfX1S5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
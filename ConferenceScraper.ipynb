{
   "cells": [
       {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
               "# Install Required Libraries\n",
               "This cell installs the necessary libraries for web scraping and data handling."
           ]
       },
       {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
               "!pip install requests beautifulsoup4 PyPDF2\n"
           ]
       },
       {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
               "# Import Libraries and Define Functions\n",
               "This cell imports the required libraries and defines functions used for scraping conference talks from the website."
           ]
       },
       {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
               "import requests\n",
               "from bs4 import BeautifulSoup\n",
               "import pandas as pd\n",
               "import re\n",
               "import unicodedata\n",
               "import time\n\n",
               "def get_soup(url):\n",
               "    \"\"\"Create a tree structure (BeautifulSoup) out of a GET request's HTML.\"\"\"\n",
               "    try:\n",
               "        r = requests.get(url, allow_redirects=True)\n",
               "        r.raise_for_status()\n",
               "        print(f\"Successfully fetched {r.url}\")\n",
               "        return BeautifulSoup(r.content, \"html5lib\")\n",
               "    except requests.RequestException as e:\n",
               "        print(f\"Error fetching {url}: {e}\")\n",
               "        return None\n\n",
               "def is_decade_page(url):\n",
               "    \"\"\"Check if a page is a decade selection page.\"\"\"\n",
               "    return bool(re.search(r\"/study/general-conference/\\d{4}\\d{4}\", url))\n\n",
               "def scrape_conference_pages(main_page_url):\n",
               "    \"\"\"Retrieve a list of URLs for each conference (year/month) from the main page.\"\"\"\n",
               "    soup = get_soup(main_page_url)\n",
               "    if soup is None:\n",
               "        print(f\"Failed to fetch content from {main_page_url}\")\n",
               "        return []\n\n",
               "    all_conference_links = []\n\n",
               "    links = [\n",
               "        \"https://www.churchofjesuschrist.org\" + a[\"href\"]\n",
               "        for a in soup.find_all(\"a\", href=True)\n",
               "        if re.search(r\"/study/general-conference/(\\d{4}/(04|10)|\\d{4}\\d{4})\", a[\"href\"])\n",
               "    ]\n\n",
               "    for link in links:\n",
               "        if is_decade_page(link):\n",
               "            decade_soup = get_soup(link)\n",
               "            if decade_soup:\n",
               "                year_links = [\n",
               "                    \"https://www.churchofjesuschrist.org\" + a[\"href\"]\n",
               "                    for a in decade_soup.find_all(\"a\", href=True)\n",
               "                    if re.search(r\"/study/general-conference/\\d{4}/(04|10)\", a[\"href\"])\n",
               "                ]\n",
               "                all_conference_links.extend(year_links)\n",
               "        else:\n",
               "            all_conference_links.append(link)\n\n",
               "    print(f\"Total conference links found: {len(all_conference_links)}\")\n",
               "    print(\"Sample conference links:\", all_conference_links[:5])\n",
               "    return all_conference_links\n\n",
               "def scrape_talk_urls(conference_url):\n",
               "    \"\"\"Retrieve a list of URLs for each talk in a specific conference.\"\"\"\n",
               "    soup = get_soup(conference_url)\n",
               "    if soup is None:\n",
               "        return []\n\n",
               "    talk_links = [\n",
               "        \"https://www.churchofjesuschrist.org\" + a[\"href\"]\n",
               "        for a in soup.find_all(\"a\", href=True)\n",
               "        if re.search(r\"/study/general-conference/\\d{4}/(04|10)/.*\", a[\"href\"])\n",
               "    ]\n\n",
               "    talk_links = list(set(talk_links))\n",
               "    talk_links = [link for link in talk_links if not link.endswith(\"session?lang=eng\")]\n\n",
               "    print(f\"Found {len(talk_links)} talk links in {conference_url}\")\n",
               "    if talk_links:\n",
               "        print(\"Sample talk links:\", talk_links[:3])\n",
               "    return talk_links\n\n",
               "def scrape_talk_data(url):\n",
               "    \"\"\"Scrapes a single talk for data such as: title, conference, calling, speaker, content.\"\"\"\n",
               "    try:\n",
               "        soup = get_soup(url)\n",
               "        if soup is None:\n",
               "            return {}\n\n",
               "        title_tag = soup.find(\"h1\", {\"id\": \"title1\"})\n",
               "        title = title_tag.text.strip() if title_tag else \"No Title Found\"\n\n",
               "        conference_tag = soup.find(\"p\", {\"class\": \"subtitle\"})\n",
               "        conference = conference_tag.text.strip() if conference_tag else \"No Conference Found\"\n\n",
               "        author_tag = soup.find(\"p\", {\"class\": \"author-name\"})\n",
               "        speaker = author_tag.text.strip() if author_tag else \"No Speaker Found\"\n\n",
               "        calling_tag = soup.find(\"p\", {\"class\": \"author-role\"})\n",
               "        calling = calling_tag.text.strip() if calling_tag else \"No Calling Found\"\n\n",
               "        content_array = soup.find(\"div\", {\"class\": \"body-block\"})\n",
               "        content = \"\\n\\n\".join(paragraph.text.strip() for paragraph in content_array.find_all(\"p\")) if content_array else \"No Content Found\"\n\n",
               "        footnotes = \"\\n\".join(\n",
               "            f\"{idx}. {note.text.strip()}\" for idx, note in enumerate(soup.find_all(\"li\", {\"class\": \"study-note\"}), start=1)\n",
               "        ) if soup.find_all(\"li\", {\"class\": \"study-note\"}) else \"No Footnotes Found\"\n\n",
               "        year = re.search(r'/(\d{4})/', url).group(1)\n",
               "        season = \"April\" if \"/04/\" in url else \"October\"\n\n",
               "        return {\n",
               "            \"title\": title,\n",
               "            \"speaker\": speaker,\n",
               "            \"calling\": calling,\n",
               "            \"conference\": conference,\n",
               "            \"year\": year,\n",
               "            \"season\": season,\n",
               "            \"url\": url,\n",
               "            \"talk\": content,\n",
               "            \"footnotes\": footnotes,\n",
               "        }\n",
               "    except Exception as e:\n",
               "        print(f\"Failed to scrape {url}: {e}\")\n",
               "        return {}\n"
           ]
       },
       {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
               "# Main Scraping Process\n",
               "This cell contains the main scraping function, which gathers all the data, processes it, and saves it to CSV and JSON files."
           ]
       },
       {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
               "def main_scrape_process():\n",
               "    main_url = \"https://www.churchofjesuschrist.org/study/general-conference?lang=eng\"\n",
               "    conference_urls = scrape_conference_pages(main_url)\n\n",
               "    all_talk_urls = []\n",
               "    for conference_url in conference_urls:\n",
               "        all_talk_urls.extend(scrape_talk_urls(conference_url))\n\n",
               "    print(f\"Total talks found: {len(all_talk_urls)}\")\n\n",
               "    conference_talks = []\n",
               "    for i, url in enumerate(all_talk_urls):\n",
               "        print(f\"Scraping talk {i+1}/{len(all_talk_urls)}: {url}\")\n",
               "        talk_data = scrape_talk_data(url)\n",
               "        if talk_data:\n",
               "            conference_talks.append(talk_data)\n\n",
               "    conference_df = pd.DataFrame(conference_talks)\n\n",
               "    for col in conference_df.columns:\n",
               "        conference_df[col] = conference_df[col].apply(lambda x: unicodedata.normalize(\"NFD\", x) if isinstance(x, str) else x)\n",
               "        conference_df[col] = conference_df[col].apply(lambda x: x.replace(\"\\t\", \"\") if isinstance(x, str) else x)\n\n",
               "    conference_df.to_csv(\"conference_talks.csv\", index=False)\n",
               "    print(\"Scraping complete. Data saved to 'conference_talks.csv'.\")\n\n",
               "    conference_df.to_json(\"conference_talks.json\", orient=\"records\", indent=4)\n",
               "    print(\"Data also saved to 'conference_talks.json'.\")\n\n",
               "start = time.time()\n",
               "main_scrape_process()\n",
               "end = time.time()\n",
               "print(f\"Total time taken: {end - start} seconds\")\n"
           ]
       },
       {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
               "# Download the CSV File\n",
               "This cell downloads the scraped data as a CSV file to your local machine."
           ]
       },
       {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
               "from google.colab import files\n",
               "files.download('conference_talks.csv')\n"
           ]
       }
   ]
}
